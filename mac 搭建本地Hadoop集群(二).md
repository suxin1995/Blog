# mac æ­å»ºHadoop+Hiveé›†ç¾¤	(äºŒ)

å‰ç½®ç¯å¢ƒï¼š

â€‹	å·²åœ¨å‰æ–‡é…ç½®å®Œæˆå¤šå°å¯å…å¯†ç™»é™†ç›¸äº’é€šä¿¡çš„é›†ç¾¤æœºå™¨

### Hadoopå®‰è£…

##### ä¸Šä¼ å¹¶è§£å‹

â€‹	å½“å‰ä½¿ç”¨ç‰ˆæœ¬ï¼šhadoop-2.7.3

â€‹	cd åˆ°èµ„æºè·¯å¾„è§£å‹ï¼š

```shell
tar zxvf hadoop-2.7.3.tar.gz
```

 	è§£å‹å®Œæˆåè¿›å…¥è·¯å¾„  hadoop-2.7.3/etc/hadoop

##### ä¿®æ”¹é…ç½®æ–‡ä»¶

1. ä¿®æ”¹core-site.xml

   ````xml
   <!-- æŒ‡å®šHDFSä¸­NameNodeçš„åœ°å€ --> 
        <property>
        <name>fs.defaultFS</name>
            <value>hdfs://master:9000</value>
        </property>
   <!-- æŒ‡å®šhadoopè¿è¡Œæ—¶äº§ç”Ÿæ–‡ä»¶çš„å­˜å‚¨ç›®å½• --> <property> 
        <name>hadoop.tmp.dir</name>
        <value>/opt/hadoop-2.7.3/tmp</value>
        </property>
   ````

2. ä¿®æ”¹hdfs-site.xml

   ````xml
   <configuration>
   <!-- è®¾ç½®dfså‰¯æœ¬æ•°ï¼Œä¸è®¾ç½®é»˜è®¤æ˜¯3ä¸ª --> 
       <property>
           <name>dfs.replication</name>
           <value>2</value>
   		</property>
   <!-- è®¾ç½®namenodeçš„ç«¯å£ -->
     	<property>
       		<name>dfs.namenode.http-address</name>
       		<value>master:50070</value>
     	</property>
   <!-- è®¾ç½®secondnameçš„ç«¯å£ --> 
       <property> 
       		<name>dfs.namenode.secondary.http-address</name>
           <value>slave1:50090</value>
       </property>
   </configuration>
   ````

3. ä¿®æ”¹mapred-site.xml

   è‹¥æ–‡ä»¶å¤¹ä¸­ä»…å­˜åœ¨mapred-site.xml.template  åˆ™éœ€è¦ç”Ÿæˆè¯¥æ–‡ä»¶

   > mv mapred-site.xml.template mapred-site.xml

   ````xml
   <configuration>
       <!-- é€šçŸ¥æ¡†æ¶MRä½¿ç”¨YARN -->
       <property>
           <name>mapreduce.framework.name</name>
           <value>yarn</value>
       </property>
   </configuration>
   ````

4. ä¿®æ”¹yarn-site.xml

   ````xml
   <configuration>
   <!-- reducerè·å–æ•°æ®çš„æ–¹å¼ --> 
     	<property> 
      				<name>yarn.nodemanager.aux-services</name>
             <value>mapreduce_shuffle</value>
       </property>
   <!-- æŒ‡å®šYARNçš„ResourceManagerçš„åœ°å€ --> 
   		<property> 
             <name>yarn.resourcemanager.hostname</name>
             <value>master</value>
       </property>
   </configuration>
   ````

5. ä¿®æ”¹Hadoop-env.sh

   åœ¨æ–‡æœ¬æœ«ç«¯å¢åŠ jdk è·¯å¾„

   > JAVA_HOME=/opt/jdk1.8.0_202

6. ä¿®æ”¹slaves

   åœ¨æ–‡ä»¶ä¸­æ ‡æ³¨å¯ç”¨èŠ‚ç‚¹

   â€‹	<img src='src/2020-12-1-6.png'>

7. ä¿®æ”¹ç¯å¢ƒé…ç½®æ–‡ä»¶

   > vi /etc/profile

   åœ¨profileæ–‡ä»¶ä¸­é…ç½®Hadoopã€jdkè·¯å¾„

   ````shell
   export JAVA_HOME=/opt/jdk1.8.0_202
   export PATH=$PATH:$JAVA_HOME/bin:$JAVA_HOME/sbin
   
   export HADOOP_HOME=/opt/hadoop-2.7.3
   export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
   ````

##### èŠ‚ç‚¹ä¼ é€

â€‹	å¤åˆ¶ä¼ é€æ–‡ä»¶

```shell
#å¤åˆ¶jdk
scp -r /opt/jdk1.8.0_202 slave1:/opt/
scp -r /opt/jdk1.8.0_181 slave2:/opt/

#å¤åˆ¶è®¾ç½®æ–‡ä»¶
scp /etc/profile slave1:/etc/
scp /etc/profile slave2:/etc/

#å¤åˆ¶hadoop
scp -r /opt/hadoop-2.7.3/ root@slave1:/opt
scp -r /opt/hadoop-2.7.3/ root@slave2:/opt
```

â€‹	åœ¨æ¯ä¸ªèŠ‚ç‚¹åˆ†åˆ«æ‰§è¡Œ ä½¿profile ç”Ÿæ•ˆ 

> Source /etc/profile



##### æ ¼å¼åŒ–é›†ç¾¤

â€‹		è‹¥é›†ç¾¤ç¬¬ä¸€æ¬¡å¯åŠ¨ï¼Œéœ€è¦æ ¼å¼åŒ–namenode

````shell
hdfs namenode -format
````

â€‹		çœ‹åˆ°INFO common.Storage: Storage directory /opt/hadoop-2.7.3/tmp/dfs/name has been successfully formatted. æˆåŠŸæ ¼å¼åŒ–å³å¯ã€‚

<img src='src/2020-12-1-5.png'>

##### å¯åŠ¨é›†ç¾¤

````shell
[root@mac hadoop-2.7.3]# start-all.sh
````

â€‹		 æ‰§è¡Œå¯åŠ¨å‘½ä»¤ï¼Œå¼€å¯hadoopé›†ç¾¤ã€‚

##### æŸ¥çœ‹è¿›ç¨‹

â€‹		å¼€å¯æˆåŠŸåï¼Œå¯åœ¨å„èŠ‚ç‚¹é€šè¿‡ jps å‘½ä»¤æŸ¥çœ‹å„èŠ‚ç‚¹çŠ¶æ€ã€‚

##### è®¿é—®ç®¡ç†é¡µé¢

â€‹		å„èŠ‚ç‚¹è¿è¡Œæ­£å¸¸åï¼Œå¯è®¿é—®webé¡µé¢ 8088ã€50070ç«¯å£æŸ¥çœ‹jobçŠ¶æ€ 

##### å‚è€ƒæ–‡æ¡£

å‚è€ƒåšå®¢é“¾æ¥1[ğŸ”—](https://www.cnblogs.com/taojietaoge/p/10803537.html)

å‚è€ƒåšå®¢é“¾æ¥2[ğŸ”—](https://www.linuxidc.com/Linux/2017-03/142051.htm)



### å®‰è£…ä¾èµ–å…ƒæ•°æ®åº“ç¯å¢ƒ

â€‹	å®‰è£…Hiveéœ€è¦æœ¬åœ°é…ç½®å…ƒæ•°æ®åº“ç¯å¢ƒã€‚æœ¬æ–‡é€‰ç”¨Mysql ä½œä¸ºè¿œç¨‹å…ƒæ•°æ®åº“ï¼Œéƒ¨ç½²åœ¨masterèŠ‚ç‚¹ä¸Šã€‚

```shell
yum install mysql
yum install mysql-devel
yum install mysql-server
```

â€‹	å‘ç°å®‰è£…mysqlå’Œmysql-develéƒ½æˆåŠŸï¼Œä½†æ˜¯å®‰è£…mysql-serverå¤±è´¥ï¼ŒæŠ¥é”™å¦‚ä¸‹ï¼š

<img src='src/2020-12-2-1.png'>

â€‹	è¿™æ˜¯ç”±äºæœ¬æœºç¯å¢ƒlinux CentOS 7 ç‰ˆæœ¬å°†MySQLæ•°æ®åº“è½¯ä»¶ä»é»˜è®¤çš„ç¨‹åºåˆ—è¡¨ä¸­ç§»é™¤ï¼Œç”¨mariadbä»£æ›¿äº†ã€‚ä½¿ç”¨å¦‚ä¸‹è§£å†³æ–¹æ³•ï¼š

```shell
wget http://dev.mysql.com/get/mysql-community-release-el7-5.noarch.rpm
rpm -ivh mysql-community-release-el7-5.noarch.rpm
yum install mysql-community-server
```

â€‹	ä¸‹è½½å®Œæˆåé‡å¯æ•°æ®åº“æœåŠ¡

```shell
service mysqld restart
```

â€‹	åˆæ¬¡è¿›å…¥mysqlï¼Œrootè´¦æˆ·æ²¡æœ‰å¯†ç 

<img src='src/2020-12-2-2.png'>

â€‹	è®¾ç½®rootç”¨æˆ·å¯†ç 

```shell
mysql> set password for 'root'@'localhost' =password('password');
Query OK, 0 rows affected (0.00 sec)
```

â€‹	ä¸ºrootç”¨æˆ·å¼€æ”¾æ‰€æœ‰åº“è¡¨æƒé™

```shell
mysql> grant all privileges on *.* to root@'%'identified by 'password';
```

â€‹	åˆ›å»º hive æ•°æ®åº“

```sql
mysql> create database hive;
```

â€‹	éªŒè¯é…ç½®ï¼Œè¿›å…¥æˆåŠŸ

> [root@master ~]# mysql -u root -p
> Enter password:



### Hiveå®‰è£…

â€‹	å½“å‰ä½¿ç”¨ç‰ˆæœ¬ï¼šhive-2.3.7

â€‹	cd åˆ°èµ„æºè·¯å¾„è§£å‹ï¼Œå¹¶å°†è§£å‹æ–‡ä»¶æ”¹åä¸ºhive-2.3.7

```shell
tar -zxfv apache-hive-2.3.7-bin.tar.gz
mv apache-hive-2.3.7-bin hive-2.3.7
```

##### ä¿®æ”¹é…ç½®æ–‡ä»¶	

â€‹	1.é…ç½®ç¯å¢ƒå˜é‡ï¼š

```shell
vi /etc/profile

# hive
export HIVE_HOME=/opt/hive-2.3.7
export PATH=$HIVE_HOME/bin:$PATH
export HIVE_CONF_DIR=$HIVE_HOME/conf
export CLASSPATH=$CLASSPATH:$HIVE_HOME/lib
```

â€‹	cd  /opt/hive-2.3.7/conf

â€‹	2.ä¿®æ”¹hive-env.sh é…ç½®ï¼š

```shell
cp hive-env.sh.template hive-env.sh 
vi hive-env.sh

HADOOP_HOME=/opt/hadoop-2.7.3
export HIVE_CONF_DIR=/opt/hive-2.3.7/conf
export HIVE_AUX_JARS_PATH=/opt/hive-2.3.7/lib
```

â€‹	3.ä¿®æ”¹hive-site.xml é…ç½®ï¼š

```shell
cp hive-default.xml.template hive-site.xml
vi hive-site.xml
```

```xml
#å…¨å±€æœç´¢ä¿®æ”¹ä»¥ä¸‹å››ä¸ªå±æ€§
<property>
    <name>javax.jdo.option.ConnectionURL</name>
    <value>jdbc:mysql://master:3306/hive?createDatabaseIfNotExist=true</value>
    <description>JDBC connect string for a JDBC metastore</description>
</property>
<property>
    <name>javax.jdo.option.ConnectionDriverName</name>
    <value>com.mysql.jdbc.Driver</value>
    <description>Driver class name for a JDBC metastore</description>
</property>
<property>
    <name>javax.jdo.option.ConnectionUserName</name>
  	#å…ƒæ•°æ®åº“username
    <value>root</value>
    <description>username to use against metastore database</description>
</property>
<property>
    <name>javax.jdo.option.ConnectionPassword</name>
  	#å…ƒæ•°æ®åº“password
    <value>*******</value>
    <description>password to use against metastore database</description>
</property>
<!--é…ç½®ç¼“å­˜ç›®å½•-->
<property>
    <name>hive.exec.local.scratchdir</name>
    <value>/opt/hive-2.3.7/iotmp</value>
    <description>Local scratch space for Hive jobs</description>
</property>
<property>
    <name>hive.downloaded.resources.dir</name>
    <value>/opt/hive-2.3.7/iotmp</value>
    <description>Temporary local directory for added resources in the remote file system.</description>
</property>

```

â€‹	4.ä¿®æ”¹ bin/hive-config.sh

```shell
export JAVA_HOME=/opt/jdk1.8.0_202
export HIVE_HOME=/opt/hive-2.3.7
export HADOOP_HOME=/opt/hadoop-2.7.3
```

##### å¯¼å…¥å…ƒæ•°æ®åº“è¿æ¥jaråŒ…	

â€‹	ä¸Šä¼ mysql-connector-java-5.1.48-bin.jar åŒ…ï¼Œå¹¶å°†å…¶ç§»åŠ¨åˆ°hive-2.3.7/bin ç›®å½•ä¸‹

```shell
mv mysql-connector-java-5.1.48 hive-2.3.7/lib/
```

##### åˆ†å‘èŠ‚ç‚¹

```shell
scp -r /opt/hive-2.3.7  root@slave1:/opt/
scp -r /opt/hive-2.3.7  root@slave2:/opt/
scp -r /opt/hive-2.3.7  root@slave3:/opt/
```

##### å­èŠ‚ç‚¹ä¿®æ”¹é…ç½®	

â€‹	åˆ†åˆ«åœ¨å„å­èŠ‚ç‚¹ä¿®æ”¹

```xaml
vi /opt/hive-2.3.7/conf/hive-site.xml

<property>  
    <name>hive.metastore.uris</name>  
    <value>thrift://master:9083</value>
    <description>Thrift URI for the remote metastore. Used by metastore client to connect to remote metastore.</description>  
</property>
```

#####	åˆå§‹åŒ–å…ƒæ•°æ®åº“

```shell
vi /opt/hive-2.3.7/bin

./schematool -dbType mysql -initSchema
```

â€‹	å¾—åˆ°æˆåŠŸåé¦ˆ

<img src='src/2020-12-2-3.png'>

##### å¯åŠ¨metastoreæœåŠ¡

```shell
hive â€“service metastore &
```


â€‹	åœ¨ master èŠ‚ç‚¹ä¸Šè¿è¡Œ jps åº”è¯¥ä¼šæœ‰RunJar è¿›ç¨‹

â€‹	æœ€ååœ¨ä¸»èŠ‚ç‚¹åŠå­èŠ‚ç‚¹è¿è¡ŒhiveéªŒè¯

##### å‚è€ƒæ–‡æ¡£

â€‹	Hiveæ–‡æ¡£å‚è€ƒé“¾æ¥[ğŸ”—](https://my.oschina.net/lvqihua/blog/3037015)



### é…ç½®Sqoopç¯å¢ƒ

##### ä¸Šä¼ å¹¶è§£å‹

â€‹	å½“å‰ä½¿ç”¨ç‰ˆæœ¬ï¼šsqoop-1.4.7.bin__hadoop-2.6.0.tar.gz

â€‹	cd åˆ°èµ„æºè·¯å¾„è§£å‹ï¼š

```shell
tar zxvf sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz
```

##### ä¿®æ”¹é…ç½®æ–‡ä»¶

â€‹	cd åˆ°è§£å‹æ–‡ä»¶è·¯å¾„  cd  sqoop-1.4.7.bin__hadoop-2.6.0/conf

â€‹	1. ç¼–è¾‘sqoop-env.sh æ–‡ä»¶

```shell
cp sqoop-env-template.sh sqoop-env.sh
vi sqoop-env.sh

export HADOOP_COMMON_HOME=/opt/hadoop-2.7.3
export HADOOP_MAPRED_HOME=/opt/hadoop-2.7.3
export HIVE_HOME=/opt/hive-2.3.7-bin
```

<img src='src/2020-12-2-5.png'>

â€‹	2. éªŒè¯å®‰è£…

```shell
cd  sqoop-1.4.7.bin__hadoop-2.6.0/bin

./sqoop version
```

##### è¿æ¥postgresqlæ•°æ®åº“

â€‹	å°†ä¸‹è½½çš„postgresql çš„jdbcé©±åŠ¨åŒ… ä»¥åŠhive/libæ–‡ä»¶å¤¹ä¸‹ hive-common-2.3.7.jaråŒ…ç½®æ”¾åœ¨sqoop/lib ä¸‹

â€‹	è¿æ¥è¿œç«¯æ•°æ®åº“å¹¶æ“ä½œï¼š

â€‹	æŸ¥çœ‹æ•°æ®åº“

```shell
./sqoop list-databases --connect jdbc:postgresql://localhost:5432 --username test --password test
```

â€‹	æŸ¥çœ‹æ•°æ®è¡¨

```shell
./sqoop list-tables --connect jdbc:postgresql://localhost:5432/test --username test --password test
```

â€‹	ä»pqå‘hiveå¯¼å…¥æ•°æ®

```shell
./sqoop import --connect jdbc:postgresql://localhost:5432/testã€Œåº“åã€ --username testã€Œæ•°æ®åº“ç”¨æˆ·ã€ --password testã€Œæ•°æ®åº“å¯†ç ã€ --table product_storageã€Œè¡¨åã€ --hive-import --hive-overwrite --hive-database=testã€Œhiveåº“åã€ --m 3ã€Œä½œä¸šèŠ‚ç‚¹æ•°ã€ -- --schema ods_v2ã€ŒæŒ‡å®šschemaã€
```



# é…ç½®sparkç¯å¢ƒ

å‰ç½®ç¯å¢ƒï¼š

â€‹	å·²åœ¨å‰æ–‡é…ç½®hadoopé›†ç¾¤æœåŠ¡å¹¶å¯ç”¨

##### è§£å‹èµ„æº

```shell
tar xvzf spark-2.4.7-bin-hadoop2.7.tgz
tar xvzf scala-2.11.12.tgz
```

##### è®¾ç½®ç¯å¢ƒå˜é‡

```shell
#scala
export SCALA_HOME=/opt/scala-2.11.12
export PATH=$SCALA_HOME/bin:$PATH

#spark
export SPARK_HOME=/opt/spark-2.4.7-bin-hadoop2.7
export PATH=$SPARK_HOME/bin:$PATH
```

##### ä¿®æ”¹é…ç½®æ–‡ä»¶

```shell
cd /opt/spark-2.4.7-bin-hadoop2.7/conf

cp spark-env.sh.template spark-env.sh
cp slaves.template	slaves

```

â€‹	é…ç½®å­èŠ‚ç‚¹ 

> vi slaves
>
> #è¾“å…¥å­èŠ‚ç‚¹host
>
> slave1
>
> slave2
>
> slave3

â€‹	å¢åŠ ç¯å¢ƒè·¯å¾„

```shell
vi spark-env.sh

export JAVA_HOME=/opt/jdk1.8.0_202
export SPARK_DIST_CLASSPATH=$(/opt/hadoop-2.7.3/bin/hadoop classpath)
export HADOOP_CONF_DIR=/opt/hadoop-2.7.3/etc/hadoop
export HADOOP_HOME=/opt/hadoop-2.7.3/

export SPARK_MASTER_HOST=master
export SPARK_MASTER_IP=master
export SPARK_WORKER_MEMORY=6g #å­èŠ‚ç‚¹åˆ†é…ä½¿ç”¨å†…å­˜

export SPARK_WORKER_CORES=3 #å­èŠ‚ç‚¹æ•°
export SCALA_HOME=/opt/scala-2.11.12
```

##### é€‚é…hive

â€‹	ä¸ºäº†å°†hiveå¹³æ»‘è¿‡æ¸¡åˆ°spark sqlï¼Œä½¿sparkå¯è¯»å–æ¥è‡ªhiveè¡¨æ•°æ®æºè¿›è¡Œè®¡ç®—ã€‚

â€‹	ä¸ºsparké…ç½®hiveçš„ä¿¡æ¯ï¼š

â€‹		1.å°†hive/confç›®å½•ä¸‹çš„hive-site.xmlæ–‡ä»¶æ‹·è´åˆ°spark/confç›®å½•ä¸‹ï¼Œä¸”ä¿®æ”¹å‚æ•°â€œhive.metastore.schema.verificationâ€çš„å€¼ä¸ºâ€œfalseâ€ã€‚é¿å…è¿›å…¥sparkå®¢æˆ·ç«¯æŠ¥ç‰ˆæœ¬ä¸åŒ¹é…é”™è¯¯ã€‚

â€‹		2.å°†hive/libä¸­çš„mysql-connector-javaé©±åŠ¨åŒ…æ‹·è´åˆ°spark/jarsä¸­		

##### å¯åŠ¨é›†ç¾¤

â€‹	å¯åŠ¨sparké›†ç¾¤ï¼Œä½¿ç”¨start-all.shå‘½ä»¤ã€‚å¯åœ¨å­èŠ‚ç‚¹é€šè¿‡jpsæŸ¥çœ‹ï¼ŒèŠ‚ç‚¹æ˜¯å¦å“åº”ã€‚

<img src='src/2020-12-4-1.png'>

##### å¯åŠ¨spark-sql

â€‹	è¿›å…¥spark/binä¸‹å¯åŠ¨spark-sql ï¼ŒæŸ¥è¯¢æ‰§è¡Œsql

```shell
spark-sql --master local[2] --jars jar/mysql-connector-java-5.1.48-bin.jar

spark-sql> 
```

â€‹	

