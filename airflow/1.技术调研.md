## airflow技术调研



**为什么用是任务调度工具**

- 业务代码和调度代码耦合严重，修改流程基本需要入侵到代码级别
- 对于定时触发的任务流程，没有一个统一管控的系统
- 多分支的复杂流程不能很好支持
- 缺少可视化的UI，不能很好追踪流程进度

------



**任务调度工具对比**

![avatar](../100.pic/4.airflow/调度工具比较.png)

---



**什么是airflow**

**airflow** 是一款开源的，分布式任务调度框架，它将一个具有上下级依赖关系的工作流，组装成一个有向无环图

---



**airflow优势**

- 业务代码和调度系统解耦，每个业务的流程代码以独立的Python脚本描述，里面定义了流程化的节点来执行业务逻辑，支持任务的热加载
- 完全支持crontab定时任务格式，可以通过crontab格式指定任务何时进行
- 支持复杂的分支条件，每个节点单独设定触发时机，如父节点全部成功时执行、任意父节点成功时执行
- 有一套完整的UI，可视化展现所有任务的状态及历史信息
- 外部依赖较少，搭建容易，仅依赖DB和rabbitmq
- python开源项目，支持扩展operate等插件，便于二次开发
- 可分布式调度：允许一个工作流的task在多台worker上同时执行
- 以有向无环图的方式构建任务依赖关系
- 工作流上每个task都是原子可重试的，一个工作流某个环节的task失败可自动或手动进行重试，不必从头开始任务

------



**airflow缺点**

- 原生airflow虽然支持分布式，但还存在一些bug，node之间的dag和task同步问题
- 缺乏合适的监控手段，需要结合kmon完善监控和报警设施
- 缺乏用户友好的编辑手段，用户需要了解airflow的原理和细节
- 大量任务运行时，调度的性能急剧下降
- 使用sqlite 数据库后端无法进行并行化

------



**airflow应用场景**

- ETL场景，可以管理数据Extract、Transform、Load等操作，以及各个Task之间的依赖，且便于完成Debug、监控和Backfill操作
- 系统运维工作，可以管理服务器端Crontab作业，以及作业之间的依赖，从而简化运维工作的复杂度，提高运维效率
- 大数据平台的任务流管理，包含数据分析、数据交换、数据报表生成与发送，等

---



**airflow任务调度原理**

首先airflow会创建一个DagFileProcessor进程池遍历dags文件夹下所有的dag文件，每个进程处理一个dag文件，产生的结果是DAGRUNS（图的状态）与taskInstance(任务实例）放入数据库中，此时taskInstance标记为Queued。

同时，shedulerjob类对象会周期的检查数据库，会将标记为queued的taskInstance放入Executor队列中，并将数据库中该任务状态标记为Scheduled

每个可用的Executor会从Executor队列中取出一个TaskInstance执行，然后将数据库中的该任务状态标记为running

当一个taskInstance执行完毕，executor会将数据库中该任务的状态标记为（完成、失败、跳过等）,按照图的任务依赖关系依次处理各任务

当一个进程处理完毕一个dag，它会为下一个dag重复上述过程。

当所有的dag文件处理完毕，airflow将进入下一个循环。若一个dag处理时间过长，在下一个循环的周期内进程池将忽略此dag，避免阻塞。

![avatar](..\100.pic\4.airflow\airflow_调度原理.png)

-----



**airflow如何分布式部署**

airflow celery采用的是消息队列的方式生产任务，分发任务和执行任务，所以想要进行分布式部署，只需要在不同机器上部署完全一样的airflow服务，在borker节点上开启airflow的webserver，scheduler，flower服务以及mysql服务和rabbitMQ服务，在其他机器上设置和borker节点上相同的配置信息，并且只开启worker服务

![avatar](..\100.pic\4.airflow\airflow_分布式结构.png)

----



**airflow如何管理**

webserver 

----



**airflow如何分区块管理**

利用Pool；这个Pool虽然不是Airflow的核心，但也跟整个Airflow的执行流程相关。任何一个Task其实都是指定了Pool这个参数的，即使没有自己指定，其实也是归结到了Default Pool这么个池子中。Pool本身是个抽象的概念，由Slot组成，可以建立任何一个Pool，指定Slot的数量。任何一个使用了这个Pool的Task Instance就需要占用一个Slot，Slot用完了，Task就处于等待状态。

----



**airflow任务sample**

**airflow**的dag文件都是用python脚本编写，总共可以分为5个步骤

```python
#【步骤一：导包】
from builtins import range
from datetime import datetime,timedelta
from airflow.models import DAG
from airflow.operators.bash_operator import BashOperator
from airflow.operators.dummy_operator import DummyOperator
from airflow.utils.dates import days_ago
 
# 【步骤二：定义默认参数】
default_args = {
    'owner': 'duoduo',  # 拥有者名称
    # 写法一：第一次开始执行的时间
    'start_date': datetime(2020, 3, 15, 10, 00),  
    # 写法二：开始时间
    #'start_date': datetime.strptime("2020-03-15 10:00:00", "%Y-%m-%d %H:%M:%S"),
    'retries': 3,  # 失败重试次数
    'retry_delay': timedelta(seconds=10)  # 失败重试间隔
}
 
# 【步骤三：定义DAG】
dag = DAG(
    dag_id='1_test',  # dag_id
    default_args=default_args,  # 指定默认参数
    
    """
    执行周期，
    schedule_interval 是任务时间设定，在dag中设置schedule_interval来定义调度周期。
    该参数可以接收cron 表达式和datetime.timedelta对象
    写法一：cron 表达式，依次是分，时，天，月，年，没有秒，
    */5 * * * *  间隔5分钟触发一次 
    */60 * * * * 间隔1小时出发一次 
    0 23 * * *   每天晚上23:00触发一次
    */60 * * * * 间隔1小时出发一次
    0 0,12 * * * 每天0:00和12:00触发
    """
    #此处代表每天23点
    #schedule_interval="0 23 * * * " 
    # 写法二：执行周期，表示每20s执行一次
    schedule_interval=timedelta(seconds=20)  
)

# 【步骤四：创建task】
#通过BashOperator定义执行bash命令的任务
a = BashOperator(  
    task_id='aaa',
    depends_on_past=True,
    bash_command='echo `date` >> /var/log/message',
    dag=dag
)

#通过BashOperator定义执行bash命令的任务
b = BashOperator(   
    task_id='bbb',
    depends_on_past=True,
    bash_command='echo 'bbb'' >> /var/log/message ,
    dag=dag
)

#通过BashOperator定义执行bash命令的任务
c = BashOperator(   
    task_id='ccc',
    depends_on_past=True,
    bash_command='echo ccc >> /var/log/message ',
    dag=dag
)
#【步骤五：设置依赖关系】
#指明c执行优先级最高，执行成功并间隔了20s以后才执行a和b，且a,b的执行顺序随机
c>>[a,b] 
```







